爬虫基本思路：
1，如果爬的全部页面就从网站首页通过xpath获取不同分类的链接，传给处理函数。如果链接参数被加密一般都是js加密所以要在network控制台去查看对应js文件
是如何生成参数来构造链接。
2.每个分类链接进入详情页后一样通过xpath或者正则表达式拿到自己要数据，有分页的话也要看参数是如何构造，拿到数据后可以持久化redis或mysql
3.一些反爬措施，像要登录后才能查看的页面得看下，网站正常登录后会生成一个什么样的认证，一般是构建一个token，可以拿到这个token去postman里测试是否
可以获取到登录信息，可以的话只要在cookie里带上这个token就行。
需要是user-agent和ip频率可以在setting里添加user池和ip代理池。
